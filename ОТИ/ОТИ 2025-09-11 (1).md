---
date: 2025-09-11
week day: " чт"
Предмет: ОТИ
Препод: Клочков Михаил Аркадьевич
Д/З: Проверить в таблице частотности символов,что сумма чисел ровна 1
---
# Конспект
>Кудрешов - теория информации 

## Основы количественной теории информации

Рассмотрим источник дискретных сообщений, пусть каждая отдельная *i-е* сообщение представляет информационный символ, выбираемый из алфавита, размерности *m* 
В двоичной системе *m = 2*, заполним матрицу
*(u1, u2, ...um)*
*(p1, p2, ...pm)*

Представим информацию как меру неопределенности источника сообщений
1. Предположим что *m = 1* (не несет полезной информации) => *I = 0*, дополнительно имеются следующие условия к *I*
2. *I(m)* должна быть не *отрицательной* и монотонно *возрастающей*
3. Функция должна обладать *свойством аддитивности*
>**Аддитивность** - это свойство величин и явлений, при котором значение целого объекта равно сумме значений его частей
4. *Кол-во* информации должно зависеть *от вероятности* (может быть частный случай, при котором все вероятности ровны *1/m*)

**Всем этим условиям удовлетворяет функция**
> I = log<sub>a</sub>(m) - логогриф от m по основанию a(где a>1)

Откуда берется *a*?
*a* - любое число больше единицы, в основе своей равняется 2-м, и такая величина называется **бит**, а если *a = e* то называется *нат* 

Можно придумать систему счисления с дробным основанием 

log<sub>e</sub> = ln - натуральный логарифм
Допустим длина сообщения = *n*, тогда кол-во всех сообщений = *N = m<sup><i>n</i></sup>*
Выводится формула *Хартли*
$$
I = \log (N) = \log (m^n) = n * \log (m)
$$
`Данная формула является частным случаем, и обычно не верна`
`Более общий случай это формула Шеннона`

Теорема Шеннона-Хартли (формула пропускной способности канала)

Эта формула определяет максимальную скорость передачи данных, которую можно достичь по каналу с аддитивным белым гауссовым шумом

![[Pasted image 20250922231653.png]] 
>Формулу Шеннона — Хартли можно переписать в виде, получив формулу, называемую границей Шеннона

***Суть***
>**Энтропия** - является мерой информационной неопределенности и выражается в битах. Чем больше неопределенность, тем больше информации мы получаем, узнав исход. 
